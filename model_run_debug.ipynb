{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7ac6a4-8c62-4ee7-8557-2edd6eb90910",
   "metadata": {},
   "source": [
    "# BERT Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341cfb89-4f3e-4228-af56-9808914b3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thisi note book is to check on the debugging for the running of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471c6c74-ac3e-429b-ac55-b007e1574342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T18:53:54.866955Z",
     "iopub.status.busy": "2023-07-15T18:53:54.866316Z",
     "iopub.status.idle": "2023-07-15T18:53:55.198065Z",
     "shell.execute_reply": "2023-07-15T18:53:55.197407Z",
     "shell.execute_reply.started": "2023-07-15T18:53:54.866928Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5a228d-065d-4508-a787-00dd2064e0b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T18:45:55.332955Z",
     "iopub.status.busy": "2023-07-15T18:45:55.332538Z",
     "iopub.status.idle": "2023-07-15T18:45:55.339009Z",
     "shell.execute_reply": "2023-07-15T18:45:55.338178Z",
     "shell.execute_reply.started": "2023-07-15T18:45:55.332910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527458bf-c4a0-4c9d-b49b-8976835e7352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T18:14:30.605186Z",
     "iopub.status.busy": "2023-07-15T18:14:30.604891Z",
     "iopub.status.idle": "2023-07-15T18:14:32.926918Z",
     "shell.execute_reply": "2023-07-15T18:14:32.926314Z",
     "shell.execute_reply.started": "2023-07-15T18:14:30.605162Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#instead now I want to load in the BERT model\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dec31e-f9f4-4c55-885e-b4d8be0500f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384de18c-66c2-4184-9131-3185b82a8080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T18:24:50.478602Z",
     "iopub.status.busy": "2023-07-15T18:24:50.477777Z",
     "iopub.status.idle": "2023-07-15T18:25:04.248900Z",
     "shell.execute_reply": "2023-07-15T18:25:04.247748Z",
     "shell.execute_reply.started": "2023-07-15T18:24:50.478571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c33a4a13e24c13b8e331b6eefb5e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73e1873b85d4c9aad20c9687831ad06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c76d19693e4ea1bc393f72e22a673d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847c8a222ac54c1cb09899fb8bab225a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd29a77b86741cc8fc35f7a7dd26054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5390541553497314}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForSequenceClassification, pipeline\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create a pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Classify some text\n",
    "text = \"This is a very good example text that we are using.\"\n",
    "result = classifier(text)\n",
    "\n",
    "# Print result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74cd945d-2e2d-4661-8e6d-15162ab001f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T19:26:08.952372Z",
     "iopub.status.busy": "2023-07-15T19:26:08.951890Z",
     "iopub.status.idle": "2023-07-15T19:27:34.999497Z",
     "shell.execute_reply": "2023-07-15T19:27:34.998737Z",
     "shell.execute_reply.started": "2023-07-15T19:26:08.952348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The scientists say that this is the first time that modern-day unicorns have been discovered in the Americas, and they are the first to date unicorns living in the Americas, says the study published in Nature Communications.\n",
      "\n",
      "The unicorns, which are native to South America, were discovered by a team of scientists from the Universidad de los Andes in Buenos Aires.\n",
      "\n",
      "But before we get into the science, an important question to ask is the age of this unicorns. The researchers say that these unicorns are indeed about 600,000 years old. The oldest known unicorn was born roughly 3,600 years ago.\n",
      "\n",
      "The study also revealed that when they came to the Americas, the unicorns used a different route, and didn’t travel through South America. Instead, they traveled through the Arctic, where they found a suitable habitat, and then they made their way to the southern plains.\n",
      "\n",
      "And the researchers, like many others, are intrigued as to why the unicorn found in El Salvador was missing.\n",
      "\n",
      "“We don’t know why it is so different,” says Luis J. Saez, a PhD student at the Universidad de los Andes.\n",
      "\n",
      "The researchers say that the reason for this is that a unicorn from South America is different from an Asian unicorn from South Asia, because a unicorn from South America was domesticated from a different species, whereas Asian unicorns were born as a wild animal, and then escaped into the wild.\n",
      "\n",
      "The researchers also say that this particular unicorn is the same animal that was present in ancient Egypt, before the Old Kingdom. And now, thanks to new insights from the scientists, the researchers say that they’ve found that the unicorn was at least 30 times its size. If left alone in a desert, all the unicorns would die.\n",
      "\n",
      "The researchers say that this was possible because the African unicorn was smaller than Asian unicorns, and it would have been able to outrun the Egyptian unicorns and survive in the desert.\n",
      "\n",
      "The researchers went to remote areas of Argentina, and they found that the unicorns weren’t born in a particular place. Instead, they found that they were able to\n",
      "As the sun set, the sky turned a brilliant orange. It was a sight to behold, as if the heavens themselves were ablaze. The Milky Way, a string of stars the size of an apple and about as bright, hung above the horizon. I could see its dark trails stretching far into the distance, and the stars in the constellation of Andromeda hung high and proud against the sky.\n",
      "\n",
      "The next morning, I woke up early. That bright orange sun was still shining through the curtains. I went down to the kitchen and picked up a cup of tea. As I walked into the garden, a beautiful white butterfly was fluttering in from the direction of the orchard; it was a male butterfly. I stepped back and watched it for a moment, wondering what it was. The male butterfly flew across to the house and went into the house. My stomach knotted up. This was not like any butterfly I had seen before.\n",
      "\n",
      "I looked through the window at the garden. A small flock of birds was eating the flowers. I was sure they were the wild turkeys I had seen on the drive that morning. There were turkeys, deer, and the beautiful peacock that I had seen on the previous night. None of the birds was as large as the peacock, but they were equally beautiful as wild animals. I stared up at the birds a few seconds, then went back to bed.\n",
      "\n",
      "I don't know how long I stayed in my bed; I didn't move or open my eyes. The sun was just rising when I heard the sound of the car coming slowly down the driveway. I sat up and checked for the pecking of birds; there were none. I felt that if there had been any, they would have been sitting on the ground waiting for me to wake up. It was a wonderful feeling, knowing that the birds that had been there the night before were still there and would have a chance to feed on the food before me.\n",
      "\n",
      "I went outside and saw the car pull up at the side of the house. I could see a man opening the door and walking towards me; I had no idea who it was.\n",
      "\n",
      "\"Rough morning,\" he yelled.\n",
      "\n",
      "I said nothing. I did not know what to say, and I was starting to think I would be unable to speak, even if I were able to.\n",
      "\n",
      "\"Hello,\" he said.\n",
      "\n",
      "\"Who are you?\"\n",
      "Creating neural networks, is the best past time on a Saturday morning in order to drive innovation in every corner of the land.\n",
      "\n",
      "As one of the major players and leaders in the field, Google doesn’t limit its scope by any means. We are constantly expanding and innovating, and our approach is more of a “bring it on” mentality. We are constantly creating new and unique tools to make our work easier, faster, and more powerful. And, of course, we keep on improving and advancing our own products (yes, that includes Google Maps!).\n",
      "\n",
      "It is in our nature to challenge ourselves, our clients, our community, and our own brains by constantly finding new ways to make our work smarter, easier, and more productive.\n",
      "\n",
      "And, while Google is far and large better at these goals than the rest of us, they still don’t have the same sense of perspective and determination needed to constantly adapt to change. For us that means working hard at being agile, innovative, and adaptive.\n",
      "\n",
      "So, in order to continuously discover new ways to improve and grow, we at Google have set a few ambitious goals for ourselves:\n",
      "\n",
      "Make Google Maps even better.\n",
      "\n",
      "Improve and expand our search capabilities.\n",
      "\n",
      "Expand our data science capabilities.\n",
      "\n",
      "Make Google Docs even better.\n",
      "\n",
      "And…\n",
      "\n",
      "Improve the way we work.\n",
      "\n",
      "With these goals in mind, all teams at Google will need to dedicate more and more of their energy to our goal of making our work smarter, easier, faster, and more productive–and they will need to be more agile–while the rest of the community tries to be agile, innovative, and adaptive.\n",
      "\n",
      "It’s all about agility. We are committed to staying agile.\n",
      "\n",
      "It is about agility, adaptability, and the relentless pursuit of continuous improvement.\n",
      "\n",
      "And it is about working hard. We work as if we are about to run out of oxygen, and it’s because we push and push and push, that we find ourselves alive.\n",
      "\n",
      "It’s about being on time. We are always 100% on time.\n",
      "\n",
      "And it’s about being agile. We stay agile.\n",
      "\n",
      "Every day we stay agile we find that we become more efficient. We find that we can reduce our work to a fraction (a little less than 1%!) of what we were doing before\n",
      "Generating '/tmp/nsys-report-cc4a.qdstrm'\n",
      "[1/7] [========================100%] report.nsys-rep\n",
      "[2/7] [========================100%] report.sqlite\n",
      "[3/7] Executing 'nvtxsum' stats report\n",
      "SKIPPED: /notebooks/GPT-J-model-comparison/report.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/7] Executing 'cudaapisum' stats report\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)               Name              \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ----------  -----------  -------------------------------\n",
      "     51.9       5269402544     954900       5518.3       5304.0       1430     1185217       2629.4  cudaLaunchKernel               \n",
      "     33.9       3448659799      39632      87017.1       9599.0       3741  1343334731    7539791.2  cudaMemcpyAsync                \n",
      "     10.5       1065644642          2  532822321.0  532822321.0  139239740   926404902  556609824.0  cudaFree                       \n",
      "      2.1        215901997      38230       5647.4       5384.0       1353      343727       7019.5  cudaStreamSynchronize          \n",
      "      0.9         87110389       1621      53738.7       2061.0          0    83659947    2077854.1  cudaStreamIsCapturing_v10000   \n",
      "      0.4         45659640        222     205674.1     182095.0       4805     1612979     135273.3  cudaMalloc                     \n",
      "      0.3         25650771       3044       8426.7       6579.5       1630       52344       4558.1  cudaMemsetAsync                \n",
      "      0.0           930006       1123        828.1        769.0        447        2848        188.1  cuGetProcAddress               \n",
      "      0.0           669108        720        929.3        849.0          0        2647        260.0  cudaStreamGetCaptureInfo_v10010\n",
      "      0.0           544131        216       2519.1       1888.0       1368        5295       1019.8  cudaEventQuery                 \n",
      "      0.0           364516        216       1687.6       1438.0          0        9953        691.8  cudaEventRecord                \n",
      "      0.0            17886         18        993.7        893.0        808        2638        420.5  cudaEventCreateWithFlags       \n",
      "      0.0             9863          3       3287.7       3134.0       2883        3846        499.6  cuInit                         \n",
      "\n",
      "[5/7] Executing 'gpukernsum' stats report\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------\n",
      "     33.1       7303622755      67152  108762.5  109231.5     45601    171808      60001.7  void gemv2T_kernel_val<int, int, float, float, float, float, (int)128, (int)16, (int)4, (int)4, (bo…\n",
      "     26.9       5941798822     102127   58180.5   44864.0     44032   1018906     112882.2  void gemv2T_kernel_val<int, int, float, float, float, float, (int)128, (int)16, (int)4, (int)4, (bo…\n",
      "     25.6       5651989755      33576  168334.2  168287.0    167039    171231        445.3  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool…\n",
      "      3.8        843789399      67008   12592.4   11809.0      3680     26112       5936.1  void at::native::<unnamed>::CatArrayBatchedCopy<float, unsigned int, (int)4, (int)128, (int)1>(T1 *…\n",
      "      1.8        402512104      68698    5859.2    5920.0      5153      7616        239.8  void at::native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T…\n",
      "      1.3        289704854      33576    8628.3    9408.0      2591     14817       3799.2  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool…\n",
      "      1.0        217274173     103747    2094.3    1824.0      1441      5728        389.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, floa…\n",
      "      1.0        213104048     102346    2082.2    2015.0      1665      8607        249.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<float>, at::deta…\n",
      "      0.7        152304394      20952    7269.2    7361.0      4511     10144       1182.1  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool…\n",
      "      0.5        106818272      33576    3181.4    3392.0      2175      4703        609.6  void at::native::unrolled_elementwise_kernel<at::native::<unnamed>::direct_copy_kernel_cuda(at::Ten…\n",
      "      0.4         96325273      33648    2862.7    2848.0      2591      3872         87.2  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.4         89471517      33648    2659.0    2687.0      2240      3584        129.6  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.3         74094925      33648    2202.1    2175.0      2048      4864        111.7  void at::native::vectorized_elementwise_kernel<(int)4, void at::native::<unnamed>::pow_tensor_scala…\n",
      "      0.3         71053825      33648    2111.7    2111.0      1697      4832         94.7  void at::native::vectorized_elementwise_kernel<(int)4, at::native::tanh_kernel_cuda(at::TensorItera…\n",
      "      0.3         64085257      17496    3662.9    3424.0      2816      6560        698.4  void <unnamed>::softmax_warp_forward<float, float, float, (int)9, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.3         63453195      33648    1885.8    1857.0      1792      8768        194.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, floa…\n",
      "      0.3         61168628      33648    1817.9    1793.0      1727      3872         80.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnSelf_add<float>, at…\n",
      "      0.2         50487253       1402   36010.9   35935.5     34656     44480        826.4  void at::native::<unnamed>::cunn_SoftMaxForward<(int)4, float, float, float, at::native::<unnamed>:…\n",
      "      0.2         43217067      12168    3551.7    3520.0      2400      4864        425.1  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool…\n",
      "      0.1         31471303        290  108521.7   54607.5     51231   1095737     106019.6  ampere_sgemm_64x32_sliced1x4_tn                                                                     \n",
      "      0.1         27449439       5608    4894.7    4832.0      4673      6368        159.7  void at::native::mbtopk::radixFindKthValues<float, unsigned int, unsigned int, (int)1>(at::cuda::de…\n",
      "      0.1         24607549       9216    2670.1    2592.0      2399      4352        301.5  void <unnamed>::softmax_warp_forward<float, float, float, (int)8, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.1         19483058       1402   13896.6   13856.0     13216     17215        350.5  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::ArgMaxOps<…\n",
      "      0.1         13364043       5608    2383.0    2336.0      2079      3041        120.8  void at::native::mbtopk::computeBlockwiseWithinKCounts<unsigned int>(T1 *, short *, unsigned int, i…\n",
      "      0.1         12311282       4608    2671.7    2655.0      2495      3680        166.3  void <unnamed>::softmax_warp_forward<float, float, float, (int)7, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.1         11332977       1402    8083.4    8064.0      7840      9984        178.9  void at::native::bitonicSortKVInPlace<float, long, (int)-2, (int)-1, at::native::GTOp<float, (bool)…\n",
      "      0.0         10906303       1402    7779.1    7745.0      7232      9473        178.8  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.0         10830834       1402    7725.3    7712.0      7457      9472        186.5  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.0         10588832       1402    7552.7    7520.0      7328      9408        185.7  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.0          9631784         25  385271.4  328830.0    304702   1763638     287431.3  ampere_sgemm_128x64_tn                                                                              \n",
      "      0.0          9602121       2804    3424.4    3584.0      3135      4448        240.9  void at_cuda_detail::cub::DeviceScanByKeyKernel<at_cuda_detail::cub::DeviceScanByKeyPolicy<at_cuda_…\n",
      "      0.0          7990870       2798    2855.9    2881.0      2591      3648        214.5  void at::native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, (int)2, (int)2, (int)-…\n",
      "      0.0          7974118       4209    1894.5    1888.0      1791      3040         74.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<long, long, bool, …\n",
      "      0.0          7773752         96   80976.6   80608.0     77919     85055       1605.8  ampere_sgemm_128x32_tn                                                                              \n",
      "      0.0          7573252       2804    2700.9    2815.0      2399      3520        218.9  void at::native::<unnamed>::CatArrayBatchedCopy<long, unsigned int, (int)2, (int)128, (int)1>(T1 *,…\n",
      "      0.0          7470341       1402    5328.3    5407.0      4543      6720        243.3  void at::native::mbtopk::gatherTopK<float, unsigned int, (int)1>(at::cuda::detail::TensorInfo<T1, T…\n",
      "      0.0          7332981         24  305540.9  305790.0    293470    319614       8202.8  ampere_sgemm_64x64_tn                                                                               \n",
      "      0.0          6054775       2804    2159.3    2176.0      2015      2944        103.5  void at::native::unrolled_elementwise_kernel<at::native::<unnamed>::direct_copy_kernel_cuda(at::Ten…\n",
      "      0.0          5426938       2804    1935.4    1984.0      1759      2496        129.6  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<long, long, long, …\n",
      "      0.0          5204418       2804    1856.1    1855.0      1759      2561         77.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, bool…\n",
      "      0.0          5104223       1402    3640.7    3616.0      3552      4416         83.4  void at::native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, (int)4, void at::nat…\n",
      "      0.0          4873964       2016    2417.6    2336.0      2303      3839        193.5  void <unnamed>::softmax_warp_forward<float, float, float, (int)6, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.0          4723334       2804    1684.5    1664.0      1599      2112         68.0  void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ReduceByKeyScanTileState<unsign…\n",
      "      0.0          4478077       1402    3194.1    3168.0      2720      3776         84.0  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<long, at::native::func_wrappe…\n",
      "      0.0          4436413       1402    3164.3    3136.0      3072      4097         79.9  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<long, at::native::func_wrappe…\n",
      "      0.0          4216233       1402    3007.3    3008.0      2879      3648         73.2  void at::native::elementwise_kernel<(int)128, (int)4, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.0          3906705       1402    2786.5    2816.0      2527      3328        100.3  void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScanPolicy<long>::Policy600, …\n",
      "      0.0          3854930       1402    2749.6    2721.0      2687      3297         75.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, floa…\n",
      "      0.0          3649471       1402    2603.0    2655.0      2079      3456        287.1  void at::native::unrolled_elementwise_kernel<at::native::<unnamed>::direct_copy_kernel_cuda(at::Ten…\n",
      "      0.0          2949105       1402    2103.5    2112.0      1855      2559         87.2  void at::native::vectorized_elementwise_kernel<(int)4, void at::native::<unnamed>::masked_fill_kern…\n",
      "      0.0          2872997       1402    2049.2    2047.0      1984      2688         67.0  void at::native::mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsig…\n",
      "      0.0          2866007       1402    2044.2    2047.0      1983      2431         59.2  void at::native::mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)                             \n",
      "      0.0          2861503       1402    2041.0    2017.0      1983      2656         56.9  void at::native::vectorized_elementwise_kernel<(int)4, void at::native::<unnamed>::masked_fill_kern…\n",
      "      0.0          2832127       1402    2020.1    2047.0      1823      2528        112.2  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnSelf_add<long>, at:…\n",
      "      0.0          2730976       1402    1947.9    1889.0      1855      2657        100.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<long>, at::detai…\n",
      "      0.0          2725251       1402    1943.8    1888.0      1823      2432        107.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<long, long, long, …\n",
      "      0.0          2678063       1402    1910.2    1887.0      1759      2401         96.5  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnOther_add<float>, a…\n",
      "      0.0          2620732       1402    1869.3    1856.0      1823      2304         54.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnOther_add<long>, at…\n",
      "      0.0          2606518       1402    1859.1    1855.0      1792      2560         72.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, bool…\n",
      "      0.0          2605122       1402    1858.1    1856.0      1792      2367         48.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<bool, bool, bool, …\n",
      "      0.0          2395412       1408    1701.3    1696.0      1631      2144         65.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<long>, at::detail::A…\n",
      "      0.0          2385980       1402    1701.8    1696.0      1633      2240         58.2  void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTileState<long, (bool)1>>(T…\n",
      "      0.0          1601659         72   22245.3   23328.0     19167     24384       2103.7  ampere_sgemm_128x128_tn                                                                             \n",
      "      0.0          1392467        456    3053.7    2976.0      2815      3712        211.3  void gemv2N_kernel<int, int, float, float, float, float, (int)128, (int)4, (int)4, (int)4, (int)1, …\n",
      "      0.0           860373        144    5974.8    5888.0      4512      6977        553.9  void at::native::<unnamed>::CatArrayBatchedCopy<float, unsigned int, (int)4, (int)64, (int)64>(T1 *…\n",
      "      0.0           765942         72   10638.1   10496.0      8543     12832       1659.1  ampere_sgemm_128x128_nn                                                                             \n",
      "      0.0           744419        312    2386.0    2240.0      2112      3264        256.1  void <unnamed>::softmax_warp_forward<float, float, float, (int)5, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.0           554910        144    3853.5    3984.0      3488      4543        275.8  void splitKreduce_kernel<(int)32, (int)16, int, float, float, float, float, (bool)1, (bool)0, (bool…\n",
      "      0.0           377404         72    5241.7    5296.5      4416      6144        547.6  void at::native::elementwise_kernel<(int)128, (int)4, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.0           213305         72    2962.6    3040.0      2528      3360        281.1  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.0           209631         48    4367.3    4447.5      3903      4929        413.0  void splitKreduce_kernel<(int)32, (int)16, int, float, float, float, float, (bool)1, (bool)1, (bool…\n",
      "      0.0            27871          6    4645.2    4719.5      3552      5535        810.1  void at::native::<unnamed>::indexSelectLargeIndex<float, long, unsigned int, (int)2, (int)2, (int)-…\n",
      "      0.0            18687          6    3114.5    3040.0      2752      3647        327.1  void at::native::<unnamed>::indexSelectSmallIndex<long, long, unsigned int, (int)2, (int)2, (int)-2…\n",
      "      0.0            12447          3    4149.0    4256.0      3456      4735        646.2  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<bool, at::native::func_wrappe…\n",
      "      0.0             3456          1    3456.0    3456.0      3456      3456          0.0  void at::native::unrolled_elementwise_kernel<at::native::BUnaryFunctor<float, float, float, at::nat…\n",
      "\n",
      "[6/7] Executing 'gpumemtimesum' stats report\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------\n",
      "     98.3        517840522  34018   15222.5     769.0       735  38038341     320995.9  [CUDA memcpy HtoD]\n",
      "      0.7          3881294   4212     921.5     897.0       832      1696         59.8  [CUDA memcpy DtoH]\n",
      "      0.5          2862019   1402    2041.4    2017.0      1951      2560         86.3  [CUDA memcpy DtoD]\n",
      "      0.5          2416801   3044     794.0     768.0       703      1600         85.2  [CUDA memset]     \n",
      "\n",
      "[7/7] Executing 'gpumemsizesum' stats report\n",
      "\n",
      "CUDA Memory Operation Statistics (by size):\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
      "   5363.102  34018     0.158     0.000     0.000   411.705        3.483  [CUDA memcpy HtoD]\n",
      "    281.841   1402     0.201     0.201     0.201     0.201        0.000  [CUDA memcpy DtoD]\n",
      "      0.338   3044     0.000     0.000     0.000     0.001        0.000  [CUDA memset]     \n",
      "      0.026   4212     0.000     0.000     0.000     0.004        0.000  [CUDA memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /notebooks/GPT-J-model-comparison/report.nsys-rep\n",
      "    /notebooks/GPT-J-model-comparison/report.sqlite\n"
     ]
    }
   ],
   "source": [
    "#Now that I have the model running as expected, I can generate the output from the Nvidia profiler to see the outputs.\n",
    "!nsys profile -t cuda,nvtx --stats=true --force-overwrite=true -o report python model_run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f1387c-ae73-4e49-8825-2ff91694f7f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T19:01:59.629074Z",
     "iopub.status.busy": "2023-07-15T19:01:59.628219Z",
     "iopub.status.idle": "2023-07-15T19:03:46.934128Z",
     "shell.execute_reply": "2023-07-15T19:03:46.933450Z",
     "shell.execute_reply.started": "2023-07-15T19:01:59.629045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbe86a4aa494c1789233acf9cc6fa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3805a925433b4aa7a10eeba762b2df13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf8186a57304196b534a59b267558cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/779k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eddd1e7ffd46a48f0eef0aa39d7a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c873dbb41e3a45dca36fee8469fc68ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6670835cbe264800b5caaf04d7bb196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b6ac93-4c2a-4da4-ab48-2b5335e1b85e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T19:03:58.928619Z",
     "iopub.status.busy": "2023-07-15T19:03:58.928131Z",
     "iopub.status.idle": "2023-07-15T19:05:23.223124Z",
     "shell.execute_reply": "2023-07-15T19:05:23.222573Z",
     "shell.execute_reply.started": "2023-07-15T19:03:58.928594Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c50fec1b6043fea295fc4481701535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading flax_model.msgpack:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, FlaxGPTNeoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "model = FlaxGPTNeoModel.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"jax\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3eda1b-7d67-43bd-8130-1bb48bf36485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-15T19:37:45.054804Z",
     "iopub.status.busy": "2023-07-15T19:37:45.054189Z",
     "iopub.status.idle": "2023-07-15T19:38:47.545961Z",
     "shell.execute_reply": "2023-07-15T19:38:47.545179Z",
     "shell.execute_reply.started": "2023-07-15T19:37:45.054776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "The scientists spent 10 years studying the strange herd, which contained a total of 16 unicorns, including 12 mature unicorns and 4 juveniles. All the unicorns spoke in perfect English, despite being in an area that was far more likely to have a language barrier.\n",
      "\n",
      "All 20 of the unicorns were born in the same place, in July 2010, as the researchers were observing. Over a three-year period, the scientists were able to record all of the unicorns’ movements.\n",
      "\n",
      "“Our research has uncovered a large, multiyear herd of unicorns that are born in a valley in the Andes Mountains and speak a language not unlike the spoken language of the common people of the area,” the researchers wrote in a scientific\n",
      "As the sun set, the sky turned a brilliant orange. It was a sight to behold, as if the heavens themselves were ablaze. The wind picked up, blowing gusts of dust into the air, swirling the clouds into dust devils, and sending them swirling and twisting, each as tall and thick as a man, with tails as sharp as knives. Lightning suddenly zapped between the clouds, sending a shower of sparks and a blinding rain of dust to race across the skies—and it was all the color, all the life, that he could comprehend.\n",
      "\n",
      "In the silence of the desert, he caught the sound of something that was not wind, yet was not silent, the soft swish of some unseen creature, or, more like it, the sound of someone struggling to keep her or his balance as she stumbled up those steps, her sandals sinking into the loose stones, her wet clothes slithering in the dry dirt.\n",
      "\n",
      "\"No, that wasn't a person\n",
      "Creating neural networks, is the best past time on a Saturday morning in order to drive innovation in every corner of the land. But that’s not to say it can’t be an exciting time for a few of the leading experts.\n",
      "\n",
      "This event is the ultimate opportunity to discuss the latest emerging technologies, and to hear cutting-edge thoughts about how we can build a better future together. It’s open to any level of person interested – from the very beginner to the very experienced.\n",
      "\n",
      "There will be no limit to the number of talks that can be presented – there is no ‘maximum time limit’ to attend.\n",
      "\n",
      "This event is hosted by a number of leading experts in the space, who are all working to ensure that the future we have to work, live, play and build with looks like the future everyone wants to live in.\n",
      "\n",
      "If you’ve got something you’re really dying to tell the world about,\n",
      "Running around San Francisco is the best past time, when taking a break from coding but it's important to always be rocking nike shoes to give yourself more time to find new things. That is why going to visit old haunts, or making a trip to San Francisco is also a great way to make it to San Francisco and enjoy the sights. There are so many places to visit that it's a wonder how you get to go every time. With that in mind I've put together some suggestions for the most places to visit in San Francisco.\n",
      "\n",
      "If you are an avid runner then it's a great place to just get out and feel the wind, or just catch the vibe of San Francisco. The beaches of the Bay are a lot like the beaches of Yosemite National Park. The wind that blows out of San Francisco makes it a great place for a run.\n",
      "\n",
      "There are a lot of people that feel that the best way to get to San Francisco is by helicopter or airplane.\n",
      "The only way to make a pizza pie, is to use the most refined flour otherwise people will not enjoy the pizza to the fullest potential. This in turn lead to health hazards and so, to avoid these hazards, one must always make sure that the best kind of flour is used for making the best pizza pie.\n",
      "\n",
      "Now, if we consider the different types of pizza in the market, we can see that there are various varieties of pizza, and each has some uniqueness. We can see that there are various varieties of pizza in the market that can be made with different dough. We can see that there are various varieties of pizza dough in the market that are used for the making of various type of pizza. Now, we can also observe that there are varied types of cheese, among various varieties of cheese used for the making of various types of pizza. Since it is a fact that there are various types of pizza in the market, we can also see that it is a matter of concern whether\n",
      "Generating '/tmp/nsys-report-8ed9.qdstrm'\n",
      "[1/7] [========================100%] report.nsys-rep\n",
      "[2/7] [========================100%] report.sqlite\n",
      "[3/7] Executing 'nvtxsum' stats report\n",
      "SKIPPED: /notebooks/GPT-J-model-comparison/report.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/7] Executing 'cudaapisum' stats report\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)               Name              \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ----------  -----------  -------------------------------\n",
      "     44.7       3523047706     577101       6104.7       5925.0          0      990963       3254.0  cudaLaunchKernel               \n",
      "     38.7       3047349078      24100     126446.0      10070.5          0  1370795040    9797132.0  cudaMemcpyAsync                \n",
      "     12.9       1018144145          2  509072072.5  509072072.5  132792235   885351910  532140049.4  cudaFree                       \n",
      "      1.7        136471004      23253       5869.0       5370.0          0      345413      10096.8  cudaStreamSynchronize          \n",
      "      1.1         87237670       1054      82768.2       2208.0          0    84878093    2614351.1  cudaStreamIsCapturing_v10000   \n",
      "      0.5         39702405        210     189059.1     176101.0       5277     1011448      81613.8  cudaMalloc                     \n",
      "      0.2         18914702       2030       9317.6       8177.5          0       63172       5316.9  cudaMemsetAsync                \n",
      "      0.0          1232958       1296        951.4        880.5          0       17339        552.0  cudaStreamGetCaptureInfo_v10010\n",
      "      0.0           888442        360       2467.9       1923.0          0        8651       1094.1  cudaEventQuery                 \n",
      "      0.0           818865       1123        729.2        690.0        565        2759        152.3  cuGetProcAddress               \n",
      "      0.0           588347        360       1634.3       1422.5          0        8465        605.9  cudaEventRecord                \n",
      "      0.0            20791         18       1155.1       1016.5        744        3137        523.7  cudaEventCreateWithFlags       \n",
      "      0.0             9358          3       3119.3       2862.0       2817        3679        485.2  cuInit                         \n",
      "\n",
      "[5/7] Executing 'gpukernsum' stats report\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------\n",
      "     34.0       4391870853      40416  108666.6  109279.5     45601    171391      60138.7  void gemv2T_kernel_val<int, int, float, float, float, float, (int)128, (int)16, (int)4, (int)4, (bo…\n",
      "     27.7       3577699293      61466   58206.2   44896.0     43968   1018686     112884.1  void gemv2T_kernel_val<int, int, float, float, float, float, (int)128, (int)16, (int)4, (int)4, (bo…\n",
      "     26.3       3404074147      20208  168451.8  168415.0    167423    171040        453.8  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool…\n",
      "      2.0        257159247      40176    6400.8    6208.5      3648     11393       1607.7  void at::native::<unnamed>::CatArrayBatchedCopy<float, unsigned int, (int)4, (int)128, (int)1>(T1 *…\n",
      "      1.9        245618079      41503    5918.1    5953.0      5216      7649        234.2  void at::native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T…\n",
      "      1.0        131482221      62675    2097.8    1825.0      1568      5504        382.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, floa…\n",
      "      1.0        129376080      61831    2092.4    2016.0      1632      8992        265.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<float>, at::deta…\n",
      "      0.7         84120557      20208    4162.7    3360.0      2655      8255       1531.1  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool…\n",
      "      0.5         69706774      19368    3599.1    3585.0      2431      4832        424.8  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, float, float, float, float, (bool…\n",
      "      0.5         62586334        580  107907.5   54271.5     51455   1092574     105574.9  ampere_sgemm_64x32_sliced1x4_tn                                                                     \n",
      "      0.4         57866266      20328    2846.6    2848.0      2592      3935        100.2  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.4         51260397      20328    2521.7    2464.0      2335      3392        120.4  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.4         50120829      20208    2480.2    2336.0      2175      3488        237.8  void at::native::unrolled_elementwise_kernel<at::native::<unnamed>::direct_copy_kernel_cuda(at::Ten…\n",
      "      0.3         44811668      20328    2204.4    2176.0      2079      4384        115.6  void at::native::vectorized_elementwise_kernel<(int)4, void at::native::<unnamed>::pow_tensor_scala…\n",
      "      0.3         43301910      20328    2130.2    2112.0      2047      4769        123.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::tanh_kernel_cuda(at::TensorItera…\n",
      "      0.3         38249336      20328    1881.6    1856.0      1791      9056        253.6  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, floa…\n",
      "      0.3         37162098      20328    1828.1    1824.0      1728      3648         96.2  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnSelf_add<float>, at…\n",
      "      0.2         30801953        847   36365.9   36224.0     35583     44352       1085.6  void at::native::<unnamed>::cunn_SoftMaxForward<(int)4, float, float, float, at::native::<unnamed>:…\n",
      "      0.2         22137363       8520    2598.3    2528.0      2399      3904        240.9  void <unnamed>::softmax_warp_forward<float, float, float, (int)8, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.2         20866755       7680    2717.0    2688.0      2175      3872        161.3  void <unnamed>::softmax_warp_forward<float, float, float, (int)7, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.1         16690851       3388    4926.5    4864.0      4704      6145        176.9  void at::native::mbtopk::radixFindKthValues<float, unsigned int, unsigned int, (int)1>(at::cuda::de…\n",
      "      0.1         11885465        847   14032.4   13984.0     13408     17152        435.4  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::ArgMaxOps<…\n",
      "      0.1          9729391         25  389175.6  329344.0    318655   1784861     290927.9  ampere_sgemm_128x64_tn                                                                              \n",
      "      0.1          8524195       3552    2399.8    2337.0      2303      4000        159.7  void <unnamed>::softmax_warp_forward<float, float, float, (int)6, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.1          8128089       3388    2399.1    2336.0      2079      3040        128.8  void at::native::mbtopk::computeBlockwiseWithinKCounts<unsigned int>(T1 *, short *, unsigned int, i…\n",
      "      0.1          7755703         96   80788.6   80576.0     78401     84800       1321.5  ampere_sgemm_128x32_tn                                                                              \n",
      "      0.1          7347219         24  306134.1  306191.0    291519    318719       8890.3  ampere_sgemm_64x64_tn                                                                               \n",
      "      0.1          6899622        847    8146.0    8097.0      7871     10048        237.3  void at::native::bitonicSortKVInPlace<float, long, (int)-2, (int)-1, at::native::GTOp<float, (bool)…\n",
      "      0.1          6658054        847    7860.7    7839.0      7647      9600        227.9  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.1          6619312        847    7815.0    7776.0      7360      9504        226.9  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.0          6425221        847    7585.9    7552.0      7392      9472        238.8  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.0          5821568       1694    3436.6    3584.0      3136      4321        242.7  void at_cuda_detail::cub::DeviceScanByKeyKernel<at_cuda_detail::cub::DeviceScanByKeyPolicy<at_cuda_…\n",
      "      0.0          4858739       1684    2885.2    2943.0      2592      3936        226.9  void at::native::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, (int)2, (int)2, (int)-…\n",
      "      0.0          4846415       2546    1903.5    1888.0      1823      3040         87.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<long, long, bool, …\n",
      "      0.0          4568981       1694    2697.2    2816.0      2399      3328        230.9  void at::native::<unnamed>::CatArrayBatchedCopy<long, unsigned int, (int)2, (int)128, (int)1>(T1 *,…\n",
      "      0.0          4556584        847    5379.7    5440.0      4576      6912        278.5  void at::native::mbtopk::gatherTopK<float, unsigned int, (int)1>(at::cuda::detail::TensorInfo<T1, T…\n",
      "      0.0          3625173       1694    2140.0    2176.0      1983      2784        120.5  void at::native::unrolled_elementwise_kernel<at::native::<unnamed>::direct_copy_kernel_cuda(at::Ten…\n",
      "      0.0          3300722       1694    1948.5    2015.0      1759      2497        130.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<long, long, long, …\n",
      "      0.0          3166907       1694    1869.5    1856.0      1791      2464         79.2  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, bool…\n",
      "      0.0          3119297        847    3682.8    3648.0      3584      4384        113.1  void at::native::<unnamed>::distribution_elementwise_grid_stride_kernel<float, (int)4, void at::nat…\n",
      "      0.0          2871445       1694    1695.1    1695.0      1631      2113         70.9  void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ReduceByKeyScanTileState<unsign…\n",
      "      0.0          2752517        847    3249.7    3232.0      3168      4000         88.2  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<long, at::native::func_wrappe…\n",
      "      0.0          2720587        847    3212.0    3200.0      3136      3999         89.6  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<long, at::native::func_wrappe…\n",
      "      0.0          2610625        847    3082.2    3072.0      2912      3776         96.4  void at::native::elementwise_kernel<(int)128, (int)4, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.0          2533021        120   21108.5   19487.5     19232     24288       2134.8  ampere_sgemm_128x128_tn                                                                             \n",
      "      0.0          2526251        840    3007.4    2976.0      2815      3840        169.0  void gemv2N_kernel<int, int, float, float, float, float, (int)128, (int)4, (int)4, (int)4, (int)1, …\n",
      "      0.0          2363074        847    2789.9    2816.0      2560      3328        116.5  void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScanPolicy<long>::Policy600, …\n",
      "      0.0          2352242        847    2777.1    2752.0      2687      3296         86.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, floa…\n",
      "      0.0          1971802        847    2328.0    2240.0      2143      2848        140.3  void at::native::unrolled_elementwise_kernel<at::native::<unnamed>::direct_copy_kernel_cuda(at::Ten…\n",
      "      0.0          1747149        847    2062.7    2048.0      1984      2433         72.5  void at::native::mbtopk::computeBlockwiseKthCounts<unsigned int>(T1 *, short *, unsigned int, unsig…\n",
      "      0.0          1744995        847    2060.2    2080.0      1855      2496         99.0  void at::native::vectorized_elementwise_kernel<(int)4, void at::native::<unnamed>::masked_fill_kern…\n",
      "      0.0          1741337        847    2055.9    2048.0      1983      2464         62.2  void at::native::mbtopk::fill<unsigned int, unsigned int>(T1 *, T1, T2)                             \n",
      "      0.0          1730723        847    2043.4    2017.0      1983      2463         71.5  void at::native::vectorized_elementwise_kernel<(int)4, void at::native::<unnamed>::masked_fill_kern…\n",
      "      0.0          1688868        847    1993.9    2016.0      1823      2752        116.5  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnSelf_add<long>, at:…\n",
      "      0.0          1653754        847    1952.5    1920.0      1855      2528         99.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<long>, at::detai…\n",
      "      0.0          1643546        847    1940.4    1888.0      1823      2720        102.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<long, long, long, …\n",
      "      0.0          1610236        847    1901.1    1856.0      1791      2432         98.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnOther_add<float>, a…\n",
      "      0.0          1600665        847    1889.8    1887.0      1823      2305         72.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnOther_add<long>, at…\n",
      "      0.0          1591607        847    1879.1    1856.0      1823      2272         68.2  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<bool, bool, bool, …\n",
      "      0.0          1586516        847    1873.1    1856.0      1823      2367         72.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, bool…\n",
      "      0.0          1463663        857    1707.9    1696.0      1631      2112         55.4  void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<long>, at::detail::A…\n",
      "      0.0          1452493        847    1714.9    1696.0      1663      2240         64.8  void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTileState<long, (bool)1>>(T…\n",
      "      0.0          1390364        240    5793.2    5888.0      4865      7072        674.8  void at::native::<unnamed>::CatArrayBatchedCopy<float, unsigned int, (int)4, (int)64, (int)64>(T1 *…\n",
      "      0.0          1349883        576    2343.5    2208.0      2143      3232        220.9  void <unnamed>::softmax_warp_forward<float, float, float, (int)5, (bool)0, (bool)0>(T2 *, const T1 …\n",
      "      0.0          1184690        120    9872.4    8736.5      8607     12991       1597.4  ampere_sgemm_128x128_nn                                                                             \n",
      "      0.0          1067899        288    3708.0    3584.0      3456      4417        249.9  void splitKreduce_kernel<(int)32, (int)16, int, float, float, float, float, (bool)1, (bool)0, (bool…\n",
      "      0.0           611867        120    5098.9    4672.5      4479      6144        596.7  void at::native::elementwise_kernel<(int)128, (int)4, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.0           404311         96    4211.6    4063.5      3872      4927        333.0  void splitKreduce_kernel<(int)32, (int)16, int, float, float, float, float, (bool)1, (bool)1, (bool…\n",
      "      0.0           341635        120    2847.0    2657.0      2559      3456        278.9  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::…\n",
      "      0.0            43938         10    4393.8    4000.0      3616      5665        770.4  void at::native::<unnamed>::indexSelectLargeIndex<float, long, unsigned int, (int)2, (int)2, (int)-…\n",
      "      0.0            29761         10    2976.1    2848.0      2593      3680        346.2  void at::native::<unnamed>::indexSelectSmallIndex<long, long, unsigned int, (int)2, (int)2, (int)-2…\n",
      "      0.0            19266          5    3853.2    3681.0      3520      4384        383.8  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<bool, at::native::func_wrappe…\n",
      "      0.0             6464          2    3232.0    3232.0      3168      3296         90.5  void at::native::unrolled_elementwise_kernel<at::native::BUnaryFunctor<float, float, float, at::nat…\n",
      "      0.0             3456          1    3456.0    3456.0      3456      3456          0.0  void at::native::vectorized_elementwise_kernel<(int)2, at::native::BUnaryFunctor<float, float, floa…\n",
      "\n",
      "[6/7] Executing 'gpumemtimesum' stats report\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------\n",
      "     98.9        512655315  20702   24763.6     769.0       735  40660495     424753.0  [CUDA memcpy HtoD]\n",
      "      0.5          2363960   2551     926.7     897.0       864      2208         71.0  [CUDA memcpy DtoH]\n",
      "      0.3          1739845    847    2054.1    2016.0      1951      2560        101.6  [CUDA memcpy DtoD]\n",
      "      0.3          1624254   2030     800.1     768.0       704      1344         84.6  [CUDA memset]     \n",
      "\n",
      "[7/7] Executing 'gpumemsizesum' stats report\n",
      "\n",
      "CUDA Memory Operation Statistics (by size):\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
      "   5363.049  20702     0.259     0.000     0.000   411.705        4.461  [CUDA memcpy HtoD]\n",
      "    170.271    847     0.201     0.201     0.201     0.201        0.000  [CUDA memcpy DtoD]\n",
      "      0.256   2030     0.000     0.000     0.000     0.001        0.000  [CUDA memset]     \n",
      "      0.016   2551     0.000     0.000     0.000     0.002        0.000  [CUDA memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /notebooks/GPT-J-model-comparison/report.nsys-rep\n",
      "    /notebooks/GPT-J-model-comparison/report.sqlite\n"
     ]
    }
   ],
   "source": [
    "#Now that I have the model running as expected, I can generate the output from the Nvidia profiler to see the outputs.\n",
    "#Running this for GPT-Neo\n",
    "!nsys profile -t cuda,nvtx --stats=true --force-overwrite=true -o report python model_run.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
